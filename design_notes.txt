First pass with just raw averaged word2vec vectors achieved 50% train accuracy. NN model was better than RF.

No notion of information about the user is currently being used, so it would be good to get an idea of the 
user's average rating. Some people are super pessimistic and others are really optimistic. How to featurize this?

The data contains Yelp reviews, businesses, users, and checkins for Phoenix, AZ. 

-In the training set:
11,537 businesses
8,282 checkin sets
43,873 users
229,907 reviews

-In the testing set:
1,205 businesses
734 checkin sets
5,105 users
22,956 reviews to predict

The training data consists of a set of reviews for businesses, each with a star rating as well as information about the user, votes, and the text itself. One of the issues with the dataset is that the testing data has the actual review text removed. 

As such, the training set (with the included reviews) was partitioned into a train and test set with disjoint data. The issue here is that there are limited training examples, so DL solutions don't have much to go off of. Transfer learning might be a good approach. 


----
Running bcn on the balanced yelp dataset yielded the following results:
max_train_acc = 69.72%
vocabulary size =  1001488
data.py parse: ../data/yelp/test.txt
dataset size =  50000
accuracy: 64.93%
f1: 64.72%

Trained on 8 epochs with batches of size 32.

----
6/13  meeting with Wasi

-Compute the precision and recall for each class 
-Which class is the performance poorest for? What kind of reviews are being misclassified?


Super simple baselines: non-neural (SVM/RF)
M1: word embedding + FF
M2: word embedding + LSTM
M3: word embedding + BCN

-Compute the average side of a review
EX: Here is an interesting test example. LSTM incorrectly classifies this, but BCN correctly classifies it. Why?
->KW will be interested in seeing error analysis

-It's possible that BCN cannot correctly classify because we might need more information. Hand-crafted features or other features.
-In transfer learning we are showing the model some domain data. For example, Rizwan's yelp reviews might be talking about aspects of food, restaurants, etc. My dataset might be talking about something else even though it's yelp data. If you retrain the model on my dataset, the chances of predicting the correct labels for my dataset should increase. Should observe some improvement compared to the pre-trained (Rizwan data) model.

-One possible thing is to split each paragraph into a set of sentences. Instead of considering it as a sequence of word, we can try to come up with a sentence representation. Get the review representation based on the sentence representation.
-Some adjectives are more useful, so we can use part of speech tags as a feature ("love" is a very important word)
-Hierarchical attention (From words to sentences, sentences to paragraphs) 

-Can try training the word embeddings as well

Q: How to integrate the POS tags into the model?
-Concatenate into word embedding vector
-POS embedding layer will be initialized from a uniform distribution
-LSTM will get a concatenated representation from word embedding layer and POS embedding layer
[word embedding] [pos embedding]
<concat>
[LSTM]

Q: How to deal with large vocab?
-Cut the vocab size down by selecting top 80000-100000 words by frequency, then train the word embeddings like this.
-If vocab size is like 700k, most of the words will be infrequent.
-replace all unknown words with token called <unk>
-set <unk> to be zero vector
-Model should update the <unk> representation 

----
6/18 test results

1) Trained on balanced data and tested on imbalanced
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  1001488
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 61.16%
f1: 60.84%
             precision    recall  f1-score   support

          0       0.56      0.75      0.64      2706
          1       0.52      0.51      0.51      3167
          2       0.50      0.55      0.53      5151
          3       0.64      0.51      0.57     11909
          4       0.67      0.74      0.71     11553

avg / total       0.61      0.61      0.61     34486


2) Trained on imbalanced data and weighted loss
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  732097
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 58.83%
f1: 58.72%
             precision    recall  f1-score   support

          0       0.64      0.61      0.63      2706
          1       0.44      0.55      0.49      3167
          2       0.49      0.42      0.45      5151
          3       0.59      0.56      0.57     11909
          4       0.67      0.70      0.68     11553

avg / total       0.59      0.59      0.59     34486


3) Trained on imbalanced data and unweighted loss
vocabulary size =  732097
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 60.51%
f1: 60.11%
             precision    recall  f1-score   support

          0       0.61      0.70      0.65      2706
          1       0.51      0.38      0.44      3167
          2       0.53      0.43      0.47      5151
          3       0.57      0.66      0.61     11909
          4       0.69      0.67      0.68     11553

avg / total       0.60      0.61      0.60     34486




