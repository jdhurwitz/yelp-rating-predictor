./test_model1.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  1001488
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 61.41%
f1: 60.84%
             precision    recall  f1-score   support

          0       0.59      0.79      0.68      2706
          1       0.50      0.51      0.51      3167
          2       0.52      0.46      0.49      5151
          3       0.64      0.52      0.57     11909
          4       0.66      0.77      0.71     11553

avg / total       0.61      0.61      0.61     34486

[[2135  384   66   28   93]
 [ 860 1620  457   97  133]
 [ 239  959 2391 1162  400]
 [ 195  208 1381 6168 3957]
 [ 184   70  283 2153 8863]]
MSE:  0.7177115351156991



./test_model3.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  1001488
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 59.60%
f1: 58.98%
             precision    recall  f1-score   support

          0       0.56      0.71      0.63      2706
          1       0.49      0.54      0.51      3167
          2       0.49      0.52      0.50      5151
          3       0.65      0.45      0.53     11909
          4       0.64      0.77      0.70     11553

avg / total       0.60      0.60      0.59     34486

[[1931  601  101   17   56]
 [ 611 1719  665   95   77]
 [ 228  827 2658 1047  391]
 [ 262  258 1651 5373 4365]
 [ 432  109  379 1761 8872]]
MSE:  0.8504610566606738


./test_model5.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  80001
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 60.53%
f1: 60.01%
             precision    recall  f1-score   support

          0       0.54      0.82      0.65      2706
          1       0.50      0.46      0.48      3167
          2       0.49      0.49      0.49      5151
          3       0.65      0.50      0.56     11909
          4       0.66      0.75      0.71     11553

avg / total       0.61      0.61      0.60     34486

[[2211  324   65   20   86]
 [1016 1468  530   65   88]
 [ 337  878 2529 1048  359]
 [ 242  186 1642 5962 3877]
 [ 311   56  376 2107 8703]]
MSE:  0.7823464594328133


./test_model6.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  80001
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 61.50%
f1: 61.95%
             precision    recall  f1-score   support

          0       0.74      0.64      0.68      2706
          1       0.48      0.69      0.56      3167
          2       0.48      0.55      0.51      5151
          3       0.60      0.62      0.61     11909
          4       0.76      0.61      0.68     11553

avg / total       0.63      0.62      0.62     34486

[[1732  853   59   39   23]
 [ 371 2179  511   78   28]
 [  81 1064 2843 1060  103]
 [  52  322 2021 7378 2136]
 [ 119  157  473 3726 7078]]
MSE:  0.6078408629588818


./test_model7.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  80001
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 60.02%
f1: 59.37%
             precision    recall  f1-score   support

          0       0.59      0.71      0.64      2706
          1       0.49      0.54      0.52      3167
          2       0.50      0.50      0.50      5151
          3       0.64      0.46      0.54     11909
          4       0.64      0.78      0.70     11553

avg / total       0.60      0.60      0.59     34486

[[1912  624   62   35   73]
 [ 618 1721  620  119   89]
 [ 204  833 2571 1108  435]
 [ 224  220 1587 5537 4341]
 [ 298   92  292 1915 8956]]
MSE:  0.7791567592646291



./test_model8.sh
maxout num classes:  5
BCN init num_units:  5
vocabulary size =  80001
data.py parse: ../data/yelp/test.txt
dataset size =  34486
accuracy: 62.27%
f1: 62.15%
             precision    recall  f1-score   support

          0       0.66      0.74      0.70      2706
          1       0.50      0.63      0.56      3167
          2       0.50      0.55      0.52      5151
          3       0.64      0.52      0.57     11909
          4       0.70      0.73      0.71     11553

avg / total       0.63      0.62      0.62     34486

[[2004  584   57   21   40]
 [ 556 2006  514   54   37]
 [ 153 1027 2824  958  189]
 [ 136  272 1906 6183 3412]
 [ 187  122  388 2399 8457]]
MSE:  0.6461752595256046

